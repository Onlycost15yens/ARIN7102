{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. QA retrivel with HealthCareMagic-100k-QA\n",
    "100k real conversations between patients and doctors from HealthCareMagic.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def load_json(filename):\n",
    "    \"\"\"Load JSON file containing medical QA data.\"\"\"\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def save_json(data, filename):\n",
    "    \"\"\"Save JSON data to file.\"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "def encode_text(text, model, tokenizer):\n",
    "    \"\"\"Encode text into BERT embeddings.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Convert to list for JSON serialization\n",
    "    return outputs.last_hidden_state[:, 0, :].numpy()[0].tolist()\n",
    "\n",
    "def preprocess_dataset_with_embeddings(json_file, model, tokenizer, batch_size=100):\n",
    "    \"\"\"\n",
    "    Process the dataset in batches, adding embeddings to each entry\n",
    "    and saving after each batch to avoid memory issues.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    data = load_json(json_file)\n",
    "    \n",
    "    # Create a backup of original file\n",
    "    backup_file = json_file + '.backup'\n",
    "    if not os.path.exists(backup_file):\n",
    "        save_json(data, backup_file)\n",
    "        print(f\"Original data backed up to {backup_file}\")\n",
    "    \n",
    "    # Track which entries already have embeddings\n",
    "    for i in tqdm(range(0, len(data), batch_size), desc=\"Processing batches\"):\n",
    "        batch = data[i:min(i+batch_size, len(data))]\n",
    "        modified = False\n",
    "        \n",
    "        for j, entry in enumerate(batch):\n",
    "            # Skip if already has embedding\n",
    "            if \"embedding\" not in entry:\n",
    "                # Generate combined text\n",
    "                text = entry[\"input\"] + \" \" + entry[\"output\"]\n",
    "                # Encode and store\n",
    "                entry[\"embedding\"] = encode_text(text, model, tokenizer)\n",
    "                modified = True\n",
    "                \n",
    "        \n",
    "        if modified:\n",
    "            # Save after each batch\n",
    "            save_json(data, json_file)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def find_best_matches_from_preprocessed(json_file, query, model, tokenizer, k=3):\n",
    "    \"\"\"Find best matches using preprocessed embeddings in the JSON file.\"\"\"\n",
    "    # Load preprocessed data with embeddings\n",
    "    data = load_json(json_file)\n",
    "    \n",
    "    # Encode query\n",
    "    query_embedding = np.array(encode_text(query, model, tokenizer))\n",
    "    \n",
    "    # Calculate similarities without loading all embeddings at once\n",
    "    similarities = []\n",
    "    for i, entry in enumerate(tqdm(data, desc=\"Calculating similarities\")):\n",
    "        if \"embedding\" in entry:\n",
    "            entry_embedding = np.array(entry[\"embedding\"])\n",
    "            # Calculate cosine similarity\n",
    "            sim = cosine_similarity([query_embedding], [entry_embedding])[0][0]\n",
    "            similarities.append((i, sim))\n",
    "    \n",
    "    # Sort by similarity\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get top-k results\n",
    "    results = []\n",
    "    for i in range(min(k, len(similarities))):\n",
    "        idx, score = similarities[i]\n",
    "        results.append((data[idx][\"output\"], score))\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = \"Dataset/HealthCareMagic-100k-QA.json\"\n",
    "query = \"My head is spinning when I stand up, but not when sitting. I also feel nauseous.\"\n",
    "k = 10\n",
    "\n",
    "# Initialize BERT model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# First time: preprocess and add embeddings to JSON file\n",
    "preprocess_dataset_with_embeddings(json_file, model, tokenizer, batch_size=100)\n",
    "\n",
    "# After preprocessing: find matches using saved embeddings\n",
    "best_matches = find_best_matches_from_preprocessed(json_file, query, model, tokenizer, k)\n",
    "\n",
    "for i, (answer, score) in enumerate(best_matches):\n",
    "    print(f\"Rank {i+1} | Score: {score:.4f}\\n{answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ClinicalBERT, LLM for medical scene\n",
    "This model was trained on a large multicenter dataset with a large corpus of 1.2B words of diverse diseases we constructed. We then utilized a large-scale corpus of EHRs from over 3 million patient records to fine tune the base language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"medicalai/ClinicalBERT\")\n",
    "model = AutoModel.from_pretrained(\"medicalai/ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. MedPub, Disease - Symptom pair dataset\n",
    "Get the Disease - Symptom pairs from MedPub Dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 download dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import urllib.request\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Base URL for PubMed baseline files\n",
    "base_url = \"https://ftp.ncbi.nlm.nih.gov/pubmed/baseline/\"\n",
    "\n",
    "# Define download directory\n",
    "download_dir = \"Dataset/pubmed_data\"\n",
    "\n",
    "# Create directory for downloaded files if it doesn't exist\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "# Download and extract files, in total 1274 files\n",
    "for i in tqdm(range(1, 10), desc=\"Processing files\"):\n",
    "    file_num = str(i).zfill(4)\n",
    "    gz_filename = f\"pubmed25n{file_num}.xml.gz\"\n",
    "    xml_filename = gz_filename[:-3]  # Remove .gz extension\n",
    "    xml_filepath = os.path.join(download_dir, xml_filename)\n",
    "    \n",
    "    # Skip if XML already exists\n",
    "    if os.path.exists(xml_filepath):\n",
    "        continue\n",
    "        \n",
    "    url = base_url + gz_filename\n",
    "    \n",
    "    try:\n",
    "        # Download gz file\n",
    "        gz_filepath = os.path.join(download_dir, gz_filename)\n",
    "        urllib.request.urlretrieve(url, gz_filepath)\n",
    "        \n",
    "        # Extract gz file\n",
    "        with gzip.open(gz_filepath, 'rb') as gz_file:\n",
    "            with open(xml_filepath, 'wb') as xml_file:\n",
    "                xml_file.write(gz_file.read())\n",
    "                \n",
    "        # Remove gz file after extraction\n",
    "        os.remove(gz_filepath)\n",
    "        \n",
    "    except Exception as e:\n",
    "        tqdm.write(f\"Error processing {gz_filename}: {str(e)}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Extract the title and abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Directory containing the XML files\n",
    "xml_dir = \"Dataset/pubmed_data\"\n",
    "\n",
    "# List to store extracted data\n",
    "papers = []\n",
    "\n",
    "# Process each XML file\n",
    "for filename in tqdm(os.listdir(xml_dir), desc=\"Processing batches\"):\n",
    "    if not filename.endswith('.xml'):\n",
    "        continue\n",
    "    \n",
    "    filepath = os.path.join(xml_dir, filename)\n",
    "    \n",
    "    try:\n",
    "        # Parse the XML file\n",
    "        tree = ET.parse(filepath)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # Extract data from each article\n",
    "        for article in root.findall('.//PubmedArticle'):\n",
    "            try:\n",
    "                # Extract PMID\n",
    "                pmid_elem = article.find('.//PMID')\n",
    "                pmid = pmid_elem.text if pmid_elem is not None else None\n",
    "                \n",
    "                # Extract abstract\n",
    "                abstract_elem = article.find('.//AbstractText')\n",
    "                abstract = abstract_elem.text if abstract_elem is not None else None\n",
    "                \n",
    "                # Only add if we have an abstract\n",
    "                if abstract:\n",
    "                    papers.append({\n",
    "                        'pmid': pmid,\n",
    "                        'abstract': abstract\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                tqdm.write(f\"Error extracting data from article: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    except Exception as e:\n",
    "        tqdm.write(f\"Error processing file {filename}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Create DataFrame from the extracted data\n",
    "papers_df = pd.DataFrame(papers)\n",
    "\n",
    "# Display the first few rows\n",
    "print(f\"Total papers extracted: {len(papers_df)}\")\n",
    "papers_df.head()\n",
    "\n",
    "# Save to CSV\n",
    "papers_df.to_csv(\"Dataset/pubmed_papers.csv\", index=False)\n",
    "print(\"Data saved to Dataset/pubmed_papers.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Generate the disease - symptom dataset\n",
    "Generate the disease - symptom dataset learning from the abstract. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
